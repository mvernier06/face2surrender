{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d0770-c613-442e-a7f8-cbb24e44e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms \n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34302fc6-7d68-43fb-b592-e7a8b19ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, annotations_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(root_dir)\n",
    "        self.attributes = self.parse_attributes(annotations_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_name)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get attributes for the current image and convert to tensor\n",
    "        attr = torch.tensor(self.attributes[self.image_filenames[idx]], dtype=torch.float32)\n",
    "\n",
    "        return image, attr\n",
    "\n",
    "    def parse_attributes(self, annotations_file):\n",
    "        attributes = {}\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            header = lines[1].strip().split()  # Skip header line\n",
    "            for line in lines[2:]:\n",
    "                parts = line.strip().split()\n",
    "                filename = parts[0]\n",
    "                attr = [int(x) for x in parts[1:]]\n",
    "                attributes[filename] = attr\n",
    "        return attributes\n",
    "\n",
    "\n",
    "# Define the root directory where CelebA dataset is stored\n",
    "celeba_root = '../data/img_align_celeba'\n",
    "annotations_file = '../data/list_attr_celeba.txt'\n",
    "\n",
    "# Define the transformations to be applied to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "    transforms.ToTensor(),         # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Create an instance of CelebADataset\n",
    "celeba_dataset = CelebADataset(root_dir=celeba_root, annotations_file=annotations_file, transform=transform)\n",
    "\n",
    "# Create a DataLoader for batching and shuffling the data\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now you can iterate over the DataLoader\n",
    "for batch_images, batch_attributes in dataloader:\n",
    "    # Process each batch here\n",
    "    print(\"Batch size:\", batch_images.size(0))\n",
    "    print(\"Shape of batch image tensor:\", batch_images.shape)  # Example: torch.Size([32, 3, 64, 64]) for RGB images\n",
    "    print(\"Shape of batch attribute tensor:\", batch_attributes.shape)  # Example: torch.Size([32, num_attributes])\n",
    "    break  # Break after printing the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede00e1f-c23d-459e-882c-bbf748102f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_channels, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 4 * 4, latent_dim)\n",
    "        \n",
    "        self.decoder_fc = nn.Linear(latent_dim, 256 * 4 * 4)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, input_channels, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.decoder_fc(z)\n",
    "        z = z.view(z.size(0), 256, 4, 4)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x, attr):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1cacb9-fe6e-49a9-a850-95fcf487d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, input_channels*64*64), x.view(-1, input_channels*64*64), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331512d5-5da0-4977-970a-c2781d73cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Define the number of dimensions for the latent space\n",
    "latent_dim = 2000\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
